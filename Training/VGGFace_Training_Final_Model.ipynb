{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Fine_tuning_smart_Daniele1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyOiUgJGpYR2","executionInfo":{"status":"ok","timestamp":1609933033355,"user_tz":-60,"elapsed":33313,"user":{"displayName":"DaniV 95","photoUrl":"","userId":"11967446588169256796"}},"outputId":"e84fb289-4e7b-4505-c93c-b1138e45913f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nmejxv_uozO7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609933037085,"user_tz":-60,"elapsed":3701,"user":{"displayName":"DaniV 95","photoUrl":"","userId":"11967446588169256796"}},"outputId":"8ca966a9-430f-4e3d-86b0-d8a2ae4bc79a"},"source":["import tensorflow as tf\n","from functools import partial\n","import matplotlib.pyplot as plt\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LgfEH4V-pOq7"},"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","BATCH_SIZE = 128\n","IMAGE_SIZE = [96, 96]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"beuPhDjYpsUy","executionInfo":{"status":"ok","timestamp":1609950909965,"user_tz":-60,"elapsed":1066,"user":{"displayName":"DaniV 95","photoUrl":"","userId":"11967446588169256796"}},"outputId":"a6e87977-7c89-4f04-d649-4acb07545870"},"source":["FILENAMES1 = tf.io.gfile.glob(\"/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/\" + \"*.tfrecords\")\n","FILENAMES2 = tf.io.gfile.glob(\"/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/\" + \"*.tfrecords\")\n","FILENAMES3 = tf.io.gfile.glob(\"/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/\" + \"*.tfrecords\")\n","FILENAMES4 = tf.io.gfile.glob(\"/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/\" + \"*.tfrecords\")\n","FILENAMES5 = tf.io.gfile.glob(\"/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group5/\" + \"*.tfrecords\")\n","FILENAMES6 = tf.io.gfile.glob(\"/content/drive/MyDrive/Final_Artificial_Vision/dataset_splitted_by_age_2000/\" + \"*.tfrecords\")\n","UTKFACE = tf.io.gfile.glob(\"/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/extrautkface/UTKFace.tfrecords\")\n","\n","FILENAMES = FILENAMES1 + FILENAMES2 + UTKFACE + FILENAMES3 +FILENAMES4\n","split_ind = int(0.77 * len(FILENAMES))\n","print(len(FILENAMES))\n","TRAINING_FILENAMES, VALID_FILENAMES = FILENAMES[:split_ind], FILENAMES[split_ind:]\n","#TEST_FILENAMES = tf.io.gfile.glob(\"/content/drive/MyDrive/Final_Artificial_Vision/test_tf_record_alignement/test_with_resize.tfrecords\")\n","\n","print(\"Train TFRecord Files:\", len(TRAINING_FILENAMES))\n","print(TRAINING_FILENAMES)\n","print(\"Validation TFRecord Files:\", len(VALID_FILENAMES))\n","print(VALID_FILENAMES)\n","#print(\"Test TFRecord Files:\", len(TEST_FILENAMES))\n","#print(TEST_FILENAMES)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["81\n","Train TFRecord Files: 62\n","['/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part1.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part10.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part2.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part3.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part4.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part5.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part6.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part7.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part8.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part9.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part11.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part12.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part13.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part14.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part15.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part16.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part17.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part18.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part19.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group1/dataset_part20.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part21.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part22.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part23.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part24.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part25.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part26.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part27.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part28.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part29.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part30.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part31.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part32.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part33.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part34.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part35.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part36.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part37.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part38.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part39.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group2/dataset_part40.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/extrautkface/UTKFace.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part41.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part42.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part43.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part44.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part45.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part46.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part47.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part48.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part49.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part50.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part51.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part52.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part53.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part54.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part55.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part56.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part57.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part58.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part59.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group3/dataset_part60.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part61.tfrecords']\n","Validation TFRecord Files: 19\n","['/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part62.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part63.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part64.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part65.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part66.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part67.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part68.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part69.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part70.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part71.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part72.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part73.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part74.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part75.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part76.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part77.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part78.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part79.tfrecords', '/content/drive/MyDrive/Final_Artificial_Vision/dataset_tf_parts/group4/dataset_part80.tfrecords']\n","Test TFRecord Files: 1\n","['/content/drive/MyDrive/Final_Artificial_Vision/test_tf_record_alignement/test_with_resize.tfrecords']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fk0m0Xnuj0xh"},"source":["!pip install git+https://github.com/rcmalli/keras-vggface.git\r\n","!pip install keras-vggface\r\n","!pip install keras_applications"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfPVY27EqssK"},"source":["def decode_image(image):\n","    #image = tf.image.decode_jpeg(image, channels=3)\n","    image = tf.io.decode_raw(image, tf.uint8)\n","    image = tf.cast(image, tf.float32)\n","    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n","    return image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LZRknx4rE22"},"source":["def read_tfrecord(example, labeled):\n","    tfrecord_format = (\n","        {\n","            \"image\": tf.io.FixedLenFeature([], tf.string),\n","            \"label_class\": tf.io.FixedLenFeature([101],tf.int64),\n","            \"label_regr_int\": tf.io.FixedLenFeature([], tf.int64),\n","        }\n","        if labeled\n","        else {\"image\": tf.io.FixedLenFeature([], tf.string),}\n","    )\n","    example = tf.io.parse_single_example(example, tfrecord_format)\n","    image = decode_image(example[\"image\"])\n","    if labeled:\n","        #label = tf.cast(example[\"label_regr_int\"], tf.int64)\n","        label = example[\"label_class\"]\n","        print(str(label))\n","        return image, label\n","    return image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b82zuMVIrbSY"},"source":["def load_dataset(filenames, labeled=True):\n","    ignore_order = tf.data.Options()\n","    ignore_order.experimental_deterministic = False  # disable order, increase speed\n","    dataset = tf.data.TFRecordDataset(\n","        filenames\n","    )  # automatically interleaves reads from multiple files\n","    dataset = dataset.with_options(\n","        ignore_order\n","    )  # uses data as soon as it streams in, rather than in its original order\n","    dataset = dataset.map(\n","        partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE\n","    )\n","    # returns a dataset of (image, label) pairs if labeled=True or just images if labeled=False\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"96EIqPGfrpbl"},"source":["def get_dataset(filenames, labeled=True):\n","    dataset = load_dataset(filenames, labeled=labeled)\n","    dataset = dataset.shuffle(2048)\n","    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PS34NXZTr7u6"},"source":["from google.colab.patches import cv2_imshow\n","train_dataset = get_dataset(TRAINING_FILENAMES)\n","valid_dataset = get_dataset(VALID_FILENAMES)\n","test_dataset = get_dataset(TEST_FILENAMES, labeled=True)\n","\n","image_batch, label_batch = next(iter(test_dataset))\n","\n","\n","def show_batch(image_batch, label_batch):\n","    plt.figure(figsize=(10, 10))\n","    for n in range(1):\n","        ax = plt.subplot(5, 5, n + 1)\n","        plt.imshow(image_batch[n] / 255.0)\n","        plt.title(str(label_batch[n]))\n","        plt.axis(\"off\")\n","\n","\n","show_batch(image_batch.numpy(), label_batch.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IXfvyVjceLa_"},"source":["# CUSTOM METRIC\r\n","\r\n","import keras.backend as K\r\n","from tensorflow.keras.metrics import mean_absolute_error\r\n","\r\n","@tf.function\r\n","def mae(y_true, y_pred): \r\n","  y_true = K.cast(y_true, y_pred.dtype)\r\n","\r\n","  ages_true = tf.map_fn(lambda true: K.argmax(true), y_true, dtype=tf.int64)\r\n","  ages_pred = tf.map_fn(lambda pred: K.argmax(pred), y_pred, dtype=tf.int64)\r\n","\r\n","  #fn_output_signature da usare nel caso map_fn non funzioni\r\n","\r\n","  return mean_absolute_error(ages_true, ages_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZQl2-P7ukSq","executionInfo":{"status":"ok","timestamp":1609961946712,"user_tz":-60,"elapsed":1441,"user":{"displayName":"DaniV 95","photoUrl":"","userId":"11967446588169256796"}},"outputId":"ae2887c2-41d8-4cdf-df7b-f35675abb01f"},"source":["#Define model VGGFace-VGG16 Classification\n","from keras_vggface.vggface import VGGFace\n","from keras.engine import  Model\n","from keras.layers import Flatten, Dense, Input, Dropout\n","vgg_model = VGGFace(weights=None, include_top=False, input_shape=(96, 96, 3))\n","\n","\"\"\"\n","# Freeze four convolution blocks\n","for layer in vgg_model.layers[:15]:\n","    layer.trainable = False\n","\n","#don't train existing weights\n","for layer in vgg_model.layers:\n","  layer.trainable = False\n","\"\"\"\n","x = vgg_model.get_layer('pool5').output\n","x = Flatten()(x) # Flatten dimensions to for use in FC layers\n","x = Dense(512, activation='relu')(x)\n","x = Dropout(0.2)(x) # Dropout layer to reduce overfitting\n","x = Dense(512, activation='relu')(x)\n","out = Dense(101, activation= 'softmax')(x) \n","transfer_model = Model(vgg_model.input, out)\n","\n","\"\"\"\n","# Loads the weights of pretraining\n","checkpoint_path = '/content/drive/MyDrive/Final_Artificial_Vision/CNN/vggFace_classificatore1.h5'\n","transfer_model.load_weights(checkpoint_path)\n","\"\"\"\n","# Loads the weights of full model\n","checkpoint_path = '/content/drive/MyDrive/Final_Artificial_Vision/CNN/vggFace_classificatore_full1.h5'\n","transfer_model.load_weights(checkpoint_path)\n","\n","# Visualize the network\n","for i, layer in enumerate(transfer_model.layers):\n","    print(i, layer.name, layer.trainable)\n","\n","\n","transfer_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 input_8 True\n","1 conv1_1 True\n","2 conv1_2 True\n","3 pool1 True\n","4 conv2_1 True\n","5 conv2_2 True\n","6 pool2 True\n","7 conv3_1 True\n","8 conv3_2 True\n","9 conv3_3 True\n","10 pool3 True\n","11 conv4_1 True\n","12 conv4_2 True\n","13 conv4_3 True\n","14 pool4 True\n","15 conv5_1 True\n","16 conv5_2 True\n","17 conv5_3 True\n","18 pool5 True\n","19 flatten_7 True\n","20 dense_21 True\n","21 dropout_7 True\n","22 dense_22 True\n","23 dense_23 True\n","Model: \"model_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_8 (InputLayer)         [(None, 96, 96, 3)]       0         \n","_________________________________________________________________\n","conv1_1 (Conv2D)             (None, 96, 96, 64)        1792      \n","_________________________________________________________________\n","conv1_2 (Conv2D)             (None, 96, 96, 64)        36928     \n","_________________________________________________________________\n","pool1 (MaxPooling2D)         (None, 48, 48, 64)        0         \n","_________________________________________________________________\n","conv2_1 (Conv2D)             (None, 48, 48, 128)       73856     \n","_________________________________________________________________\n","conv2_2 (Conv2D)             (None, 48, 48, 128)       147584    \n","_________________________________________________________________\n","pool2 (MaxPooling2D)         (None, 24, 24, 128)       0         \n","_________________________________________________________________\n","conv3_1 (Conv2D)             (None, 24, 24, 256)       295168    \n","_________________________________________________________________\n","conv3_2 (Conv2D)             (None, 24, 24, 256)       590080    \n","_________________________________________________________________\n","conv3_3 (Conv2D)             (None, 24, 24, 256)       590080    \n","_________________________________________________________________\n","pool3 (MaxPooling2D)         (None, 12, 12, 256)       0         \n","_________________________________________________________________\n","conv4_1 (Conv2D)             (None, 12, 12, 512)       1180160   \n","_________________________________________________________________\n","conv4_2 (Conv2D)             (None, 12, 12, 512)       2359808   \n","_________________________________________________________________\n","conv4_3 (Conv2D)             (None, 12, 12, 512)       2359808   \n","_________________________________________________________________\n","pool4 (MaxPooling2D)         (None, 6, 6, 512)         0         \n","_________________________________________________________________\n","conv5_1 (Conv2D)             (None, 6, 6, 512)         2359808   \n","_________________________________________________________________\n","conv5_2 (Conv2D)             (None, 6, 6, 512)         2359808   \n","_________________________________________________________________\n","conv5_3 (Conv2D)             (None, 6, 6, 512)         2359808   \n","_________________________________________________________________\n","pool5 (MaxPooling2D)         (None, 3, 3, 512)         0         \n","_________________________________________________________________\n","flatten_7 (Flatten)          (None, 4608)              0         \n","_________________________________________________________________\n","dense_21 (Dense)             (None, 512)               2359808   \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_22 (Dense)             (None, 512)               262656    \n","_________________________________________________________________\n","dense_23 (Dense)             (None, 101)               51813     \n","=================================================================\n","Total params: 17,388,965\n","Trainable params: 17,388,965\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H_x4Oj32Eqv_"},"source":["from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import ModelCheckpoint\n","lr_reduce = ReduceLROnPlateau(monitor='val_mae', factor=0.2, patience=2, verbose=1, mode='min')\n","checkpoint = ModelCheckpoint('/content/drive/MyDrive/Final_Artificial_Vision/CNN/vggFace_classificatore_full1.h5', monitor= 'val_mae', mode= 'min', save_weights_only=False, save_best_only = True, verbose= 1)\n","\n","from tensorflow.keras import layers, models, Model, optimizers\n","\n","transfer_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=optimizers.SGD(lr=(0.0001), momentum=0.9), metrics=[tf.keras.metrics.CategoricalAccuracy(), mae])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9SIUEUIXu7rF"},"source":["#Train the model VGGFace-VGG16 Classification\n","history = transfer_model.fit(train_dataset, epochs=5, validation_data=valid_dataset, callbacks=[checkpoint, lr_reduce])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hE41ZBf5gjiY"},"source":["#Plot the mae details\n","plt.plot(history.history['mae'])\n","plt.plot(history.history['val_mae'])"],"execution_count":null,"outputs":[]}]}